# Multi-Agent Translation Analysis System

## Overview

This project implements a multi-agent translation pipeline involving English, French, and Hebrew. It evaluates the impact of spelling errors on semantic drift through a three-stage translation chain.

The system uses Ollama local LLMs to perform sequential translations while injecting controlled spelling errors to measure semantic preservation across languages.

## Features

- **Three Translation Agents** (EN â†’ FR â†’ HE â†’ EN) using Ollama LLMs
- **Error Injection System** with configurable error rates (0% to 50%)
- **Semantic Distance Calculation** using Sentence-Transformers embeddings
- **Command-Line Interface** with 3 main commands (translate, experiment, analyze)
- **Comprehensive Test Suite** with 95% code coverage (22 tests)
- **Advanced Analysis** with Jupyter notebook for statistical insights
- **Professional Documentation** including PRD, ADRs, and guides

## Installation

##Prerequisites
Python 3.10 or higher
pip package manager
Git (for cloning)
Install Ollama
On Linux/WSL:
bash
curl -fsSL https://ollama.com/install.sh | sh

On macOS:
bash
brew install ollama

On Windows:
Download from ollama.com/download
Pull the required model:
bash
ollama pull llama3.2:1b

Verify installation:
bash
ollama --version
ollama list  # Should show llama3.2:1b

### Setup Steps

1. Clone or download the repository:
```bash
cd ~/LLM_HW3
```

2. Create a virtual environment:
```bash
python3 -m venv venv
source venv/bin/activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Start Ollama in a separate terminal:
```bash
ollama serve
```

5. Verify setup:
```bash
python src/cli.py --help
```

## Usage

### Command 1: Single Translation

Translate a single sentence with optional error injection:

```bash
python src/cli.py translate "The quick brown fox jumps over the lazy dog" --error-rate 0.2
```

Parameters:
- `text`: The sentence to translate
- `--error-rate`: Error rate (0.0 to 0.5, default: 0.0)
- `--seed`: Random seed for reproducibility (default: 42)

### Command 2: Run Experiment

Execute a full experiment across multiple error rates:

```bash
python src/cli.py experiment
```

This will:
- Load test sentences from config/config.yaml
- Run translations for each error rate (0%, 10%, 20%, ..., 50%)
- Perform 3 runs per configuration for statistical validity
- Save results to results/experiment.json

### Command 3: Analyze Results

Generate visualizations and statistics from experiment results:

```bash
python src/cli.py analyze results/experiment.json
```

This will generate:
- error_impact_detailed.png (main results graph)
- distance_distributions.png (distribution histograms)
- distance_boxplot.png (box plot comparison)

## Project Structure

```
LLM_HW3/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ base_agent.py          # Abstract base class
â”‚   â”‚   â”œâ”€â”€ translator_agent.py    # Ollama-based translator
â”‚   â”‚   â””â”€â”€ agent_chain.py         # Orchestration logic
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â””â”€â”€ similarity.py          # Sentence embedding & distance
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ error_injection.py     # Spelling error generation
â”‚   â””â”€â”€ cli.py                     # Command-line interface
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_agents.py
â”‚   â”œâ”€â”€ test_error_injection.py
â”‚   â””â”€â”€ test_embeddings.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml                # Experiment configuration
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ experiment_analysis.ipynb  # Data analysis & visualization
â”œâ”€â”€ ADR/
â”‚   â”œâ”€â”€ ADR-001-agent-architecture.md
â”‚   â”œâ”€â”€ ADR-002-embedding-model.md
â”‚   â””â”€â”€ ADR-003-error-injection-strategy.md
â”œâ”€â”€ results/
â”‚   â””â”€â”€ experiment.json            # Experiment output
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ PRD.md                         # Product requirements
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ .gitignore                     # Git configuration
â””â”€â”€ pytest.ini                     # Test configuration
```

## Configuration

Edit `config/config.yaml` to customize experiments:

```yaml
ollama:
  model: llama3.2:3b
  base_url: http://localhost:11434
  temperature: 0.3

experiment:
  error_rates: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]
  num_runs: 3
  seed: 42

test_sentences:
  - "Your test sentence here"
  - "Another sentence to test"
```

## Requirements

### System Requirements
- Python 3.10+
- RAM: 4GB minimum
- Ollama: Running locally on port 11434

### Python Dependencies
- typer (CLI framework)
- rich (terminal output)
- sentence-transformers (embeddings)
- transformers (NLP models)
- scipy (statistics)
- pandas (data processing)
- matplotlib (visualization)
- PyYAML (configuration)
- pytest (testing)

All dependencies are listed in `requirements.txt`


## ðŸ’° Cost Breakdown

This project uses **Ollama** (free, local LLMs), so there are **no API costs**.

| Component | Provider | Cost |
|-----------|----------|------|
| Translation (llama3.2:1b) | Ollama (local) | $0.00 |
| Embeddings (all-MiniLM-L6-v2) | sentence-transformers | $0.00 |
| **Total** | - | **$0.00** |

### Resource Usage
- **Disk Space:** ~2GB for Ollama models
- **RAM:** ~4GB during experiment runs
- **Runtime:** ~20-25 minutes for 18 experiments (6 error rates Ã— 3 sentences)
- **CPU:** Utilizes 4-8 cores during translation



## Testing

Run the test suite:

```bash
pytest -v
```

Run with coverage report:

```bash
pytest --cov=src --cov-report=html
```

View coverage report:

```bash
open htmlcov/index.html
```

## Documentation

### Key Documents
- **PRD.md** - Product requirements and specifications
- **ADR/** - Architecture decision records
  - ADR-001: Multi-agent orchestration pattern
  - ADR-002: Embedding model selection
  - ADR-003: Error injection methodology
- **notebooks/experiment_analysis.ipynb** - Statistical analysis and insights

## Troubleshooting

### "Ollama not running"
```bash
ollama serve
```

### "Config file not found"
```bash
# Ensure you're running from project root
cd ~/LLM_HW3
```

### "Permission denied" on push
Create a Personal Access Token on GitHub and use it as password.

## Results Interpretation

The experiments measure how spelling errors affect semantic preservation across the translation chain:

- **Distance â‰ˆ 0.0**: Perfect semantic preservation
- **Distance â‰ˆ 0.1-0.3**: Good semantic preservation despite errors
- **Distance > 0.5**: Significant semantic drift

Linear analysis shows the relationship between error rate and semantic distance.

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## License

MIT License - See LICENSE file for details

## Author

Roie Gilad  
Email: roie@example.com  
Date: November 2025

## Citation

If you use this project in your research, please cite:

```bibtex
@software{gilad2025multilingual,
  title={Multi-Agent Translation Analysis System},
  author={Gilad, Roie},
  year={2025},
  url={https://github.com/roiegilad8/LLM_Agent_Orchestration_HW3}
}
```

## Acknowledgments

- Ollama for local LLM capabilities
- Hugging Face for Sentence Transformers
- The open-source community for excellent libraries
